<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Path to AGI | Roland's blog</title>
<meta name=keywords content="ML"><meta name=description content="
An optimizer that always works

Minimizes the loss function without any hyper parameters


Efficient video understanding

Essentially solving the long context problem
Constant memory, constant disk. The model has to keep on compressing to meet the constant memory constraint.
Memory could be the hidden state in RNN, or in the weights like in test time training.
Disk is queried using RAG. Conceptually, agentic AI can go to the queried time in a video player, or navigate to any file in any folder.

AI outputs “Go to time 1m20s in the video”, and then the future input tokens would be the frames of the video starting at 1m20s. The model would output nothing at first, looking at the video, until it eventually gives another instruction.




Good RL algorithm that can achieve superhuman in Go, StarCraft, math, and code

Some combination of deep seek r1 and the classical RL in Go and StarCraft


RL environments with proper reward functions.

Agents is an application of 4
An example RL environment is using the computer, where it gets a reward if it achieves some specified task.
Robotics is also an application of 4.
The environment is the real world, where the reward is some specified task such as moving some object, cleaning the house, or cooking a meal. Touching someone without consent would be a negative reward.



Lingering questions"><meta name=author content="Roland Gao"><link rel=canonical href=http://localhost:1313/posts/my-first-post/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/my-first-post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/my-first-post/"><meta property="og:site_name" content="Roland's blog"><meta property="og:title" content="Path to AGI"><meta property="og:description" content=" An optimizer that always works Minimizes the loss function without any hyper parameters Efficient video understanding Essentially solving the long context problem Constant memory, constant disk. The model has to keep on compressing to meet the constant memory constraint. Memory could be the hidden state in RNN, or in the weights like in test time training. Disk is queried using RAG. Conceptually, agentic AI can go to the queried time in a video player, or navigate to any file in any folder. AI outputs “Go to time 1m20s in the video”, and then the future input tokens would be the frames of the video starting at 1m20s. The model would output nothing at first, looking at the video, until it eventually gives another instruction. Good RL algorithm that can achieve superhuman in Go, StarCraft, math, and code Some combination of deep seek r1 and the classical RL in Go and StarCraft RL environments with proper reward functions. Agents is an application of 4 An example RL environment is using the computer, where it gets a reward if it achieves some specified task. Robotics is also an application of 4. The environment is the real world, where the reward is some specified task such as moving some object, cleaning the house, or cooking a meal. Touching someone without consent would be a negative reward. Lingering questions"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-18T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-18T00:00:00+00:00"><meta property="article:tag" content="ML"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Path to AGI"><meta name=twitter:description content="
An optimizer that always works

Minimizes the loss function without any hyper parameters


Efficient video understanding

Essentially solving the long context problem
Constant memory, constant disk. The model has to keep on compressing to meet the constant memory constraint.
Memory could be the hidden state in RNN, or in the weights like in test time training.
Disk is queried using RAG. Conceptually, agentic AI can go to the queried time in a video player, or navigate to any file in any folder.

AI outputs “Go to time 1m20s in the video”, and then the future input tokens would be the frames of the video starting at 1m20s. The model would output nothing at first, looking at the video, until it eventually gives another instruction.




Good RL algorithm that can achieve superhuman in Go, StarCraft, math, and code

Some combination of deep seek r1 and the classical RL in Go and StarCraft


RL environments with proper reward functions.

Agents is an application of 4
An example RL environment is using the computer, where it gets a reward if it achieves some specified task.
Robotics is also an application of 4.
The environment is the real world, where the reward is some specified task such as moving some object, cleaning the house, or cooking a meal. Touching someone without consent would be a negative reward.



Lingering questions"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Path to AGI","item":"http://localhost:1313/posts/my-first-post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Path to AGI","name":"Path to AGI","description":" An optimizer that always works Minimizes the loss function without any hyper parameters Efficient video understanding Essentially solving the long context problem Constant memory, constant disk. The model has to keep on compressing to meet the constant memory constraint. Memory could be the hidden state in RNN, or in the weights like in test time training. Disk is queried using RAG. Conceptually, agentic AI can go to the queried time in a video player, or navigate to any file in any folder. AI outputs “Go to time 1m20s in the video”, and then the future input tokens would be the frames of the video starting at 1m20s. The model would output nothing at first, looking at the video, until it eventually gives another instruction. Good RL algorithm that can achieve superhuman in Go, StarCraft, math, and code Some combination of deep seek r1 and the classical RL in Go and StarCraft RL environments with proper reward functions. Agents is an application of 4 An example RL environment is using the computer, where it gets a reward if it achieves some specified task. Robotics is also an application of 4. The environment is the real world, where the reward is some specified task such as moving some object, cleaning the house, or cooking a meal. Touching someone without consent would be a negative reward. Lingering questions\n","keywords":["ML"],"articleBody":" An optimizer that always works Minimizes the loss function without any hyper parameters Efficient video understanding Essentially solving the long context problem Constant memory, constant disk. The model has to keep on compressing to meet the constant memory constraint. Memory could be the hidden state in RNN, or in the weights like in test time training. Disk is queried using RAG. Conceptually, agentic AI can go to the queried time in a video player, or navigate to any file in any folder. AI outputs “Go to time 1m20s in the video”, and then the future input tokens would be the frames of the video starting at 1m20s. The model would output nothing at first, looking at the video, until it eventually gives another instruction. Good RL algorithm that can achieve superhuman in Go, StarCraft, math, and code Some combination of deep seek r1 and the classical RL in Go and StarCraft RL environments with proper reward functions. Agents is an application of 4 An example RL environment is using the computer, where it gets a reward if it achieves some specified task. Robotics is also an application of 4. The environment is the real world, where the reward is some specified task such as moving some object, cleaning the house, or cooking a meal. Touching someone without consent would be a negative reward. Lingering questions\nHow does the model learn continuously? Is it really possible that all the learning is in the hidden states in RNN, without changes to the weights? Probably not, humans have only one continuous training stage; while models have the train stage and the inference stage. How to make models have one continuous training stage? The solution is possibly some continuous update of the weights and also the hidden states. If I tell the model to read a book, does it learn as much as humans do? It can probably memorize more of it, but does it understand as much? Next token prediction is clearly not enough, especially in math textbooks. In a math textbook, the model needs to come up with its own train set with questions and answers. Textbook problems can be easily converted, but expositions (implicit problems) can be harder to convert. Then it can do RL to learn from it. The model needs to specify when to learn and when to not learn. For example, when it’s converting the dataset into Q and A and trying 100 CoT paths on a problem and verifying if the CoT answer is correct, it’s only doing inference. Then, it can do the actual learning to teach itself to have the specified output given some input. Basically deepseek R1 but the learning structure and when to learn is decided by the model itself instead of manually set up. Model’s decision to do gradient descent can be implemented as a function call. If AI can solve all the previous problems, perhaps it can learn from data as efficiently as humans. If it still cannot, we have to keep on iterating to close this gap. Even if AI cannot learn as efficiently, it can still achieve super human intelligence by using significantly more data and compute than humans. But if the AI can learn as efficiently, its intelligence will be even more super human than before. In fact, we could define the intelligence quotient as the data and compute efficiency. And intelligence = intelligence quotient x data x compute. ","wordCount":"571","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-05-18T00:00:00Z","dateModified":"2025-05-18T00:00:00Z","author":{"@type":"Person","name":"Roland Gao"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/my-first-post/"},"publisher":{"@type":"Organization","name":"Roland's blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Roland's blog (Alt + H)">Roland's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archive/ title=Archive><span>Archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Path to AGI</h1><div class=post-meta><span title='2025-05-18 00:00:00 +0000 UTC'>Date: May 18, 2025</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;571 words&nbsp;·&nbsp;Roland Gao</div></header><div class=post-content><ol><li>An optimizer that always works<ol><li>Minimizes the loss function without any hyper parameters</li></ol></li><li>Efficient video understanding<ol><li>Essentially solving the long context problem</li><li>Constant memory, constant disk. The model has to keep on compressing to meet the constant memory constraint.</li><li>Memory could be the hidden state in RNN, or in the weights like in test time training.</li><li>Disk is queried using RAG. Conceptually, agentic AI can go to the queried time in a video player, or navigate to any file in any folder.<ol><li>AI outputs “Go to time 1m20s in the video”, and then the future input tokens would be the frames of the video starting at 1m20s. The model would output nothing at first, looking at the video, until it eventually gives another instruction.</li></ol></li></ol></li><li>Good RL algorithm that can achieve superhuman in Go, StarCraft, math, and code<ol><li>Some combination of deep seek r1 and the classical RL in Go and StarCraft</li></ol></li><li>RL environments with proper reward functions.<ol><li>Agents is an application of 4</li><li>An example RL environment is using the computer, where it gets a reward if it achieves some specified task.</li><li>Robotics is also an application of 4.</li><li>The environment is the real world, where the reward is some specified task such as moving some object, cleaning the house, or cooking a meal. Touching someone without consent would be a negative reward.</li></ol></li></ol><p>Lingering questions</p><ol><li>How does the model learn continuously?<ol><li>Is it really possible that all the learning is in the hidden states in RNN, without changes to the weights?<ol><li>Probably not, humans have only one continuous training stage; while models have the train stage and the inference stage. How to make models have one continuous training stage?</li><li>The solution is possibly some continuous update of the weights and also the hidden states.</li></ol></li></ol></li><li>If I tell the model to read a book, does it learn as much as humans do?<ol><li>It can probably memorize more of it, but does it understand as much?</li><li>Next token prediction is clearly not enough, especially in math textbooks.</li><li>In a math textbook, the model needs to come up with its own train set with questions and answers. Textbook problems can be easily converted, but expositions (implicit problems) can be harder to convert.<ol><li>Then it can do RL to learn from it.</li><li>The model needs to specify when to learn and when to not learn.<ol><li>For example, when it’s converting the dataset into Q and A and trying 100 CoT paths on a problem and verifying if the CoT answer is correct, it’s only doing inference. Then, it can do the actual learning to teach itself to have the specified output given some input. Basically deepseek R1 but the learning structure and when to learn is decided by the model itself instead of manually set up. Model’s decision to do gradient descent can be implemented as a function call.</li></ol></li></ol></li></ol></li><li>If AI can solve all the previous problems, perhaps it can learn from data as efficiently as humans. If it still cannot, we have to keep on iterating to close this gap.<ol><li>Even if AI cannot learn as efficiently, it can still achieve super human intelligence by using significantly more data and compute than humans.</li><li>But if the AI can learn as efficiently, its intelligence will be even more super human than before.</li><li>In fact, we could define the intelligence quotient as the data and compute efficiency. And intelligence = intelligence quotient x data x compute.</li></ol></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ml/>ML</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/eval/><span class=title>« Prev</span><br><span>Browsing Evals</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Path to AGI on x" href="https://x.com/intent/tweet/?text=Path%20to%20AGI&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy-first-post%2f&amp;hashtags=ML"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Path to AGI on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy-first-post%2f&amp;title=Path%20to%20AGI&amp;summary=Path%20to%20AGI&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy-first-post%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Path to AGI on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy-first-post%2f&title=Path%20to%20AGI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Roland's blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>